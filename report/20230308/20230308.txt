 1.统计最大深度 a.大的非终结符的最大深度 利用istoolmax的construct root b.根据每一个操作符 统计操作符下属的每一个非终结符的最大深度（但是这样编码基因的时候有点点麻烦）root->subnodes可以直接求出来这个深度 adone 限深还没有处理好（需要定位到对应的ntInt@10之类的 可以处理） 1.还没有处理到内部的subnodes 2.还没有支持限深之后得到适应度函数
 2.编码基因(至少要保留一个ite） 给出遗传算法 适应度函数用啥呢（num(i)/numorigin(i)-1)
 交叉：不一定是对半给出基因吧 随机获得父本母本的基因
 变异：变异概率？0.05
 3.根据基因给出对应DSL文件 不同编号的测试文件有不同的start symbol和常变量 利用istool对不同编号的文件的反应处理start symbol（常变量自然完成？变量是否真的ok？） 脚本控制 ./runsygus 28 在同一个文件里给不同任务的output做比对 并获取最后的分数（记录每个基因的得分） 获取cmd标准输出(利用文件？） done
 4.选择出较优的几个细粒度筛选

能得到适应度函数标准下比原来优一部分的解 进行了一些细粒度检验

总结：
当前是一个半成品版本
1.限制深度完全没有应用 
主要原因是先裁剪后读入的方式会让一些start类型非string的不好跑起来（接口实现问题）
次要问题是没有想好到底应该用哪一种（只对nonterminal进行限制 或者 对每个rule的subnonterminal进行限制）
2.string的ite产生式保留（之前提到过的不得不保有的限制）
3.只进行了runs = 20，种群数量K = 50，基因长度L = 34的一次实验
4.在3的实验结果中表现最好的一些基因都去掉了str.replace，简单的验证发现许多任务表现都变差了，主观的选择了一个相对表现较好且没有去掉str.replace的进行了细粒度检验  验证fitness function的可靠性
5.细粒度检验目前只检查了obe的表现 且没有重复多轮取平均值 具体见表格 表格中的测试样例有一部分是训练集包含的{1,5,6,7,8,9,10,12,14,17,18,20,23,25,27,29,30,31,32,33,34,35,36,37,38,13,24,28,15} 其余的是训练集不包含的
6.适应度函数   -26 -25
if value1[i] == 0:
    res = res + 100
else:
    res = res + (value1[i]/value2[i]-1)
简单神经网络替换此部分

100个解
(num(i)-numori(i))/numori(i)

对比Dreamcoder的后续工作 拿它得到的DSL？？
今天讲的一篇 两篇论文readlist

给人做library 谢冰老师的学生已经毕业了（曹xx）

TODO：
0.读两篇论文考虑对比的方式
1.重复实验 实验完整化自动化
2.解决限制深度（？）
3.检查fitness function是否有问题
4.改fitness function（神经网络 feature）

----------------------------------------------
0.library learning 见微信评论 另一篇还没看完（应该也是同样的topic 第一篇评论有和它的对比）
1.实验自动化完整化的过程：
	a.获取candidate基因后 生成所有测试集在该基因下的DSL编码（hardcode工作）done
	b.写好脚本运行所有测试生成对应文件夹 done
	c.导出实验结果自动化？ 生成excel todo
2.暂时先不管
3.检查一下原本得分最高的文件完整跑下来的结果 0110001101101000110000000101110000 去掉了replace会在哪些地方效果好吗 整体结果见genesfile文件夹里的结果i_.out（timeout=500）
	如对于训练集中的1，obe会使用str.replace来达到更快的效果 而maxflash提供的100个训练程序里并没有用到str.replace 可能是求解器的偏好问题(12也相同）还观察到训练集以外的有不少str.replace 所以那些在训练集里去掉str.replace的在内部得分挺高的
	maxflash：str.++(str.substr(Param0,0,1),str.substr(Param0,str.indexof(Param0," ",0),str.len(Param0)))
	obe：str.replace(Param0,str.substr(Param0,1,str.indexof(Param0," ",1))," ")
	(constraint (= (f "John Doe") "J Doe"))
	(constraint (= (f "Mayur Naik") "M Naik"))
	(constraint (= (f "Nimit Singh") "N Singh"))
4.尝试利用yzy的简单网络结构 pytorch入门在看

再跑了40runs：现有的adhoc的适应度函数到后来区分度不够高 考虑使用类似于加速比的适应度函数
0110001101100010110100000101110000
0110001101100010110000000101110000 这俩是一样的因为唯一不同的那一位不发挥作用
都把str.replace去掉了
应该不用进一步细化实验

加速比适应度下的选择： res = res + value2[i]/value1[i] 然后再求平均
几乎全都去掉了str.replace

结论：受训练集的一部分限制 maxflash得到的结果用于训练时可能有一些在obe上表现就不太好 （trainset的改进？）

对原本的上一次得到的基因在maxflash上做一次细粒度检验：（受时间限制有一些没有完全跑满timeout就kill了）
见origin中的im.out和i_m.out
附带原有的得分高的见genesfile中的i_m.out(还没跑）
中途感想：变差的很明显 有什么改进的example先看看的吗


eusolver也跑了一部分 很多效果不错
见origin中的ie.out和i_e.out
有不少ie.out是killed而非timeout







